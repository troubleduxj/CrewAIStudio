# CrewAI Studio æ•°æ®åº“è®¾è®¡æ–‡æ¡£

## ğŸ“‹ æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº† CrewAI Studio é¡¹ç›®çš„æ•°æ®åº“è®¾è®¡ï¼ŒåŒ…æ‹¬æ•°æ®æ¨¡å‹ã€å…³ç³»å›¾ã€ç´¢å¼•ç­–ç•¥ã€çº¦æŸè§„åˆ™å’Œæ•°æ®è¿ç§»æ–¹æ¡ˆã€‚

---

## ğŸ—ï¸ æ•°æ®åº“æ¶æ„æ¦‚è§ˆ

### æŠ€æœ¯æ ˆ
- **æ•°æ®åº“å¼•æ“**: PostgreSQL 14+
- **ORMæ¡†æ¶**: SQLAlchemy 2.0
- **è¿ç§»å·¥å…·**: Alembic
- **è¿æ¥æ± **: SQLAlchemy Pool
- **å¤‡ä»½ç­–ç•¥**: pg_dump + å®šæ—¶å¤‡ä»½

### æ•°æ®åº“å‘½åè§„èŒƒ
- **è¡¨å**: å°å†™å­—æ¯ + ä¸‹åˆ’çº¿ï¼Œå¤æ•°å½¢å¼ (å¦‚: `agents`, `tasks`)
- **å­—æ®µå**: å°å†™å­—æ¯ + ä¸‹åˆ’çº¿ (å¦‚: `created_at`, `max_execution_time`)
- **ç´¢å¼•å**: `idx_{table}_{column(s)}` (å¦‚: `idx_agents_status`)
- **å¤–é”®å**: `fk_{table}_{referenced_table}_{column}` (å¦‚: `fk_tasks_agents_agent_id`)
- **çº¦æŸå**: `ck_{table}_{constraint_desc}` (å¦‚: `ck_agents_status_valid`)

---

## ğŸ“Š æ ¸å¿ƒæ•°æ®æ¨¡å‹

### 1. åŸºç¡€æ¨¡å‹ (BaseModel)

```sql
-- æ‰€æœ‰è¡¨çš„åŸºç¡€å­—æ®µ
CREATE TABLE base_fields (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
```

**å­—æ®µè¯´æ˜**:
- `id`: ä¸»é”®ï¼Œä½¿ç”¨UUIDç¡®ä¿å…¨å±€å”¯ä¸€æ€§
- `created_at`: åˆ›å»ºæ—¶é—´ï¼Œå¸¦æ—¶åŒºä¿¡æ¯
- `updated_at`: æ›´æ–°æ—¶é—´ï¼Œè‡ªåŠ¨æ›´æ–°

### 2. ä»£ç†æ¨¡å‹ (Agent)

```sql
CREATE TABLE agents (
    -- åŸºç¡€å­—æ®µ
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    
    -- åŸºæœ¬ä¿¡æ¯
    name VARCHAR(255) NOT NULL,
    description TEXT,
    role VARCHAR(255) NOT NULL,
    goal TEXT NOT NULL,
    backstory TEXT,
    
    -- ç±»å‹å’ŒçŠ¶æ€
    type VARCHAR(50) DEFAULT 'STANDARD',
    status VARCHAR(20) DEFAULT 'ACTIVE' CHECK (status IN ('ACTIVE', 'INACTIVE', 'TRAINING', 'ERROR')),
    
    -- LLMé…ç½®
    llm_config JSONB DEFAULT '{}',
    model VARCHAR(100),
    temperature DECIMAL(3,2) DEFAULT 0.7 CHECK (temperature >= 0 AND temperature <= 2),
    max_tokens INTEGER DEFAULT 1000 CHECK (max_tokens > 0),
    
    -- å·¥å…·å’Œèƒ½åŠ›
    tools JSONB DEFAULT '[]',
    capabilities JSONB DEFAULT '[]',
    
    -- æ‰§è¡Œé…ç½®
    max_execution_time INTEGER DEFAULT 300 CHECK (max_execution_time > 0),
    allow_delegation BOOLEAN DEFAULT false,
    verbose BOOLEAN DEFAULT false,
    
    -- æç¤ºè¯
    system_prompt TEXT,
    custom_prompts JSONB DEFAULT '{}',
    
    -- æ€§èƒ½ç»Ÿè®¡
    total_executions INTEGER DEFAULT 0,
    successful_executions INTEGER DEFAULT 0,
    failed_executions INTEGER DEFAULT 0,
    avg_execution_time DECIMAL(10,2) DEFAULT 0,
    
    -- çº¦æŸ
    CONSTRAINT ck_agents_temperature_range CHECK (temperature >= 0 AND temperature <= 2),
    CONSTRAINT ck_agents_execution_stats CHECK (
        total_executions >= 0 AND 
        successful_executions >= 0 AND 
        failed_executions >= 0 AND
        successful_executions + failed_executions <= total_executions
    )
);
```

### 3. ä»»åŠ¡æ¨¡å‹ (Task)

```sql
CREATE TABLE tasks (
    -- åŸºç¡€å­—æ®µ
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    
    -- åŸºæœ¬ä¿¡æ¯
    name VARCHAR(255) NOT NULL,
    description TEXT NOT NULL,
    type VARCHAR(50) DEFAULT 'STANDARD',
    
    -- çŠ¶æ€ç®¡ç†
    status VARCHAR(20) DEFAULT 'PENDING' CHECK (
        status IN ('PENDING', 'RUNNING', 'COMPLETED', 'FAILED', 'CANCELLED', 'PAUSED')
    ),
    priority INTEGER DEFAULT 5 CHECK (priority >= 1 AND priority <= 10),
    
    -- æ•°æ®
    input_data JSONB DEFAULT '{}',
    output_data JSONB DEFAULT '{}',
    context JSONB DEFAULT '{}',
    
    -- æ‰§è¡Œé…ç½®
    max_execution_time INTEGER DEFAULT 300 CHECK (max_execution_time > 0),
    retry_count INTEGER DEFAULT 0 CHECK (retry_count >= 0),
    max_retries INTEGER DEFAULT 3 CHECK (max_retries >= 0),
    
    -- å…³è”
    assigned_agent_id UUID REFERENCES agents(id) ON DELETE SET NULL,
    parent_task_id UUID REFERENCES tasks(id) ON DELETE CASCADE,
    workflow_id UUID, -- å¤–é”®å°†åœ¨workflowè¡¨åˆ›å»ºåæ·»åŠ 
    
    -- ä¾èµ–å…³ç³»
    dependencies JSONB DEFAULT '[]', -- å­˜å‚¨ä¾èµ–ä»»åŠ¡çš„IDæ•°ç»„
    
    -- æ‰§è¡Œä¿¡æ¯
    started_at TIMESTAMP WITH TIME ZONE,
    completed_at TIMESTAMP WITH TIME ZONE,
    execution_time DECIMAL(10,2),
    error_message TEXT,
    logs JSONB DEFAULT '[]',
    
    -- æ ‡ç­¾å’Œå…ƒæ•°æ®
    tags JSONB DEFAULT '[]',
    metadata JSONB DEFAULT '{}',
    
    -- çº¦æŸ
    CONSTRAINT ck_tasks_execution_time_positive CHECK (execution_time >= 0),
    CONSTRAINT ck_tasks_completion_logic CHECK (
        (status = 'COMPLETED' AND completed_at IS NOT NULL) OR
        (status != 'COMPLETED' AND (completed_at IS NULL OR completed_at IS NOT NULL))
    ),
    CONSTRAINT ck_tasks_start_complete_order CHECK (
        started_at IS NULL OR completed_at IS NULL OR started_at <= completed_at
    )
);
```

### 4. å·¥ä½œæµæ¨¡å‹ (Workflow)

```sql
CREATE TABLE workflows (
    -- åŸºç¡€å­—æ®µ
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    
    -- åŸºæœ¬ä¿¡æ¯
    name VARCHAR(255) NOT NULL,
    description TEXT,
    version VARCHAR(50) DEFAULT '1.0.0',
    
    -- çŠ¶æ€å’Œç±»å‹
    status VARCHAR(20) DEFAULT 'DRAFT' CHECK (
        status IN ('DRAFT', 'ACTIVE', 'INACTIVE', 'ARCHIVED')
    ),
    type VARCHAR(50) DEFAULT 'SEQUENTIAL',
    execution_mode VARCHAR(20) DEFAULT 'SYNC' CHECK (
        execution_mode IN ('SYNC', 'ASYNC')
    ),
    
    -- å·¥ä½œæµå®šä¹‰
    workflow_definition JSONB NOT NULL DEFAULT '{}',
    
    -- é…ç½®
    agent_configs JSONB DEFAULT '{}',
    task_configs JSONB DEFAULT '{}',
    
    -- æ‰§è¡Œé…ç½®
    max_execution_time INTEGER DEFAULT 1800 CHECK (max_execution_time > 0),
    retry_policy JSONB DEFAULT '{"max_retries": 3, "retry_delay": 5}',
    error_handling VARCHAR(20) DEFAULT 'STOP' CHECK (
        error_handling IN ('STOP', 'CONTINUE', 'RETRY')
    ),
    
    -- è°ƒåº¦é…ç½®
    scheduling JSONB DEFAULT '{}',
    trigger_conditions JSONB DEFAULT '{}',
    
    -- æ‰§è¡ŒçŠ¶æ€
    current_step INTEGER DEFAULT 0,
    total_steps INTEGER DEFAULT 0,
    
    -- ç»Ÿè®¡ä¿¡æ¯
    total_executions INTEGER DEFAULT 0,
    successful_executions INTEGER DEFAULT 0,
    failed_executions INTEGER DEFAULT 0,
    avg_execution_time DECIMAL(10,2) DEFAULT 0,
    
    -- æ ‡ç­¾å’Œå…ƒæ•°æ®
    tags JSONB DEFAULT '[]',
    metadata JSONB DEFAULT '{}',
    
    -- çº¦æŸ
    CONSTRAINT ck_workflows_step_range CHECK (
        current_step >= 0 AND current_step <= total_steps
    ),
    CONSTRAINT ck_workflows_execution_stats CHECK (
        total_executions >= 0 AND 
        successful_executions >= 0 AND 
        failed_executions >= 0 AND
        successful_executions + failed_executions <= total_executions
    )
);

-- æ·»åŠ workflowå¤–é”®åˆ°tasksè¡¨
ALTER TABLE tasks ADD CONSTRAINT fk_tasks_workflows_workflow_id 
    FOREIGN KEY (workflow_id) REFERENCES workflows(id) ON DELETE CASCADE;
```

### 5. æ‰§è¡Œè®°å½•æ¨¡å‹ (Execution)

```sql
CREATE TABLE executions (
    -- åŸºç¡€å­—æ®µ
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    
    -- æ‰§è¡Œç±»å‹
    execution_type VARCHAR(20) NOT NULL CHECK (
        execution_type IN ('AGENT', 'TASK', 'WORKFLOW')
    ),
    
    -- å…³è”å®ä½“
    agent_id UUID REFERENCES agents(id) ON DELETE CASCADE,
    task_id UUID REFERENCES tasks(id) ON DELETE CASCADE,
    workflow_id UUID REFERENCES workflows(id) ON DELETE CASCADE,
    
    -- æ‰§è¡ŒçŠ¶æ€
    status VARCHAR(20) DEFAULT 'PENDING' CHECK (
        status IN ('PENDING', 'RUNNING', 'COMPLETED', 'FAILED', 'CANCELLED', 'TIMEOUT')
    ),
    
    -- æ‰§è¡Œæ•°æ®
    input_data JSONB DEFAULT '{}',
    output_data JSONB DEFAULT '{}',
    context JSONB DEFAULT '{}',
    
    -- æ—¶é—´ä¿¡æ¯
    started_at TIMESTAMP WITH TIME ZONE,
    completed_at TIMESTAMP WITH TIME ZONE,
    execution_time DECIMAL(10,2),
    timeout_at TIMESTAMP WITH TIME ZONE,
    
    -- é”™è¯¯ä¿¡æ¯
    error_code VARCHAR(50),
    error_message TEXT,
    error_details JSONB DEFAULT '{}',
    
    -- æ‰§è¡Œæ—¥å¿—
    logs JSONB DEFAULT '[]',
    
    -- æ€§èƒ½æŒ‡æ ‡
    cpu_usage DECIMAL(5,2),
    memory_usage DECIMAL(10,2),
    
    -- å…ƒæ•°æ®
    metadata JSONB DEFAULT '{}',
    
    -- çº¦æŸ
    CONSTRAINT ck_executions_entity_reference CHECK (
        (execution_type = 'AGENT' AND agent_id IS NOT NULL AND task_id IS NULL AND workflow_id IS NULL) OR
        (execution_type = 'TASK' AND task_id IS NOT NULL AND agent_id IS NULL AND workflow_id IS NULL) OR
        (execution_type = 'WORKFLOW' AND workflow_id IS NOT NULL AND agent_id IS NULL AND task_id IS NULL)
    ),
    CONSTRAINT ck_executions_time_logic CHECK (
        started_at IS NULL OR completed_at IS NULL OR started_at <= completed_at
    ),
    CONSTRAINT ck_executions_execution_time_positive CHECK (execution_time >= 0)
);
```

### 6. ä»»åŠ¡ä¾èµ–å…³ç³»è¡¨ (Task Dependencies)

```sql
CREATE TABLE task_dependencies (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    
    -- ä¾èµ–å…³ç³»
    task_id UUID NOT NULL REFERENCES tasks(id) ON DELETE CASCADE,
    depends_on_task_id UUID NOT NULL REFERENCES tasks(id) ON DELETE CASCADE,
    
    -- ä¾èµ–ç±»å‹
    dependency_type VARCHAR(20) DEFAULT 'FINISH_TO_START' CHECK (
        dependency_type IN ('FINISH_TO_START', 'START_TO_START', 'FINISH_TO_FINISH', 'START_TO_FINISH')
    ),
    
    -- çº¦æŸ
    CONSTRAINT ck_task_deps_no_self_reference CHECK (task_id != depends_on_task_id),
    CONSTRAINT uk_task_dependencies UNIQUE (task_id, depends_on_task_id)
);
```

---

## ğŸ”— å®ä½“å…³ç³»å›¾

```mermaid
erDiagram
    AGENTS {
        uuid id PK
        varchar name
        text description
        varchar role
        text goal
        text backstory
        varchar type
        varchar status
        jsonb llm_config
        varchar model
        decimal temperature
        integer max_tokens
        jsonb tools
        jsonb capabilities
        integer max_execution_time
        boolean allow_delegation
        boolean verbose
        text system_prompt
        jsonb custom_prompts
        timestamp created_at
        timestamp updated_at
    }
    
    TASKS {
        uuid id PK
        varchar name
        text description
        varchar type
        varchar status
        integer priority
        jsonb input_data
        jsonb output_data
        jsonb context
        integer max_execution_time
        integer retry_count
        integer max_retries
        uuid assigned_agent_id FK
        uuid parent_task_id FK
        uuid workflow_id FK
        jsonb dependencies
        timestamp started_at
        timestamp completed_at
        decimal execution_time
        text error_message
        jsonb logs
        jsonb tags
        jsonb metadata
        timestamp created_at
        timestamp updated_at
    }
    
    WORKFLOWS {
        uuid id PK
        varchar name
        text description
        varchar version
        varchar status
        varchar type
        varchar execution_mode
        jsonb workflow_definition
        jsonb agent_configs
        jsonb task_configs
        integer max_execution_time
        jsonb retry_policy
        varchar error_handling
        jsonb scheduling
        jsonb trigger_conditions
        integer current_step
        integer total_steps
        integer total_executions
        integer successful_executions
        integer failed_executions
        decimal avg_execution_time
        jsonb tags
        jsonb metadata
        timestamp created_at
        timestamp updated_at
    }
    
    EXECUTIONS {
        uuid id PK
        varchar execution_type
        uuid agent_id FK
        uuid task_id FK
        uuid workflow_id FK
        varchar status
        jsonb input_data
        jsonb output_data
        jsonb context
        timestamp started_at
        timestamp completed_at
        decimal execution_time
        timestamp timeout_at
        varchar error_code
        text error_message
        jsonb error_details
        jsonb logs
        decimal cpu_usage
        decimal memory_usage
        jsonb metadata
        timestamp created_at
        timestamp updated_at
    }
    
    TASK_DEPENDENCIES {
        uuid id PK
        uuid task_id FK
        uuid depends_on_task_id FK
        varchar dependency_type
        timestamp created_at
    }
    
    AGENTS ||--o{ TASKS : "assigned_to"
    WORKFLOWS ||--o{ TASKS : "contains"
    TASKS ||--o{ TASKS : "parent_child"
    TASKS ||--o{ TASK_DEPENDENCIES : "has_dependencies"
    TASKS ||--o{ TASK_DEPENDENCIES : "depended_by"
    AGENTS ||--o{ EXECUTIONS : "executes"
    TASKS ||--o{ EXECUTIONS : "executes"
    WORKFLOWS ||--o{ EXECUTIONS : "executes"
```

---

## ğŸ“ˆ ç´¢å¼•ç­–ç•¥

### 1. ä¸»é”®ç´¢å¼• (è‡ªåŠ¨åˆ›å»º)
```sql
-- æ‰€æœ‰è¡¨çš„ä¸»é”®ç´¢å¼•è‡ªåŠ¨åˆ›å»º
-- PRIMARY KEY indexes are automatically created
```

### 2. å¤–é”®ç´¢å¼•
```sql
-- Tasksè¡¨å¤–é”®ç´¢å¼•
CREATE INDEX idx_tasks_assigned_agent_id ON tasks(assigned_agent_id);
CREATE INDEX idx_tasks_parent_task_id ON tasks(parent_task_id);
CREATE INDEX idx_tasks_workflow_id ON tasks(workflow_id);

-- Executionsè¡¨å¤–é”®ç´¢å¼•
CREATE INDEX idx_executions_agent_id ON executions(agent_id);
CREATE INDEX idx_executions_task_id ON executions(task_id);
CREATE INDEX idx_executions_workflow_id ON executions(workflow_id);

-- Task Dependenciesè¡¨å¤–é”®ç´¢å¼•
CREATE INDEX idx_task_deps_task_id ON task_dependencies(task_id);
CREATE INDEX idx_task_deps_depends_on_task_id ON task_dependencies(depends_on_task_id);
```

### 3. çŠ¶æ€å’ŒæŸ¥è¯¢ç´¢å¼•
```sql
-- çŠ¶æ€ç´¢å¼•
CREATE INDEX idx_agents_status ON agents(status);
CREATE INDEX idx_tasks_status ON tasks(status);
CREATE INDEX idx_workflows_status ON workflows(status);
CREATE INDEX idx_executions_status ON executions(status);

-- æ—¶é—´ç´¢å¼•
CREATE INDEX idx_agents_created_at ON agents(created_at);
CREATE INDEX idx_tasks_created_at ON tasks(created_at);
CREATE INDEX idx_workflows_created_at ON workflows(created_at);
CREATE INDEX idx_executions_created_at ON executions(created_at);
CREATE INDEX idx_executions_started_at ON executions(started_at);
CREATE INDEX idx_executions_completed_at ON executions(completed_at);

-- å¤åˆç´¢å¼•
CREATE INDEX idx_tasks_status_priority ON tasks(status, priority DESC);
CREATE INDEX idx_executions_type_status ON executions(execution_type, status);
CREATE INDEX idx_tasks_agent_status ON tasks(assigned_agent_id, status);
```

### 4. JSONBå­—æ®µç´¢å¼•
```sql
-- JSONBå­—æ®µçš„GINç´¢å¼•
CREATE INDEX idx_agents_tools_gin ON agents USING GIN(tools);
CREATE INDEX idx_agents_capabilities_gin ON agents USING GIN(capabilities);
CREATE INDEX idx_tasks_tags_gin ON tasks USING GIN(tags);
CREATE INDEX idx_workflows_tags_gin ON workflows USING GIN(tags);
CREATE INDEX idx_executions_logs_gin ON executions USING GIN(logs);

-- ç‰¹å®šJSONBè·¯å¾„ç´¢å¼•
CREATE INDEX idx_agents_llm_model ON agents((llm_config->>'model'));
CREATE INDEX idx_workflows_execution_mode ON workflows(execution_mode);
```

### 5. å…¨æ–‡æœç´¢ç´¢å¼•
```sql
-- å…¨æ–‡æœç´¢ç´¢å¼•
CREATE INDEX idx_agents_search ON agents USING GIN(
    to_tsvector('english', coalesce(name, '') || ' ' || coalesce(description, '') || ' ' || coalesce(role, ''))
);

CREATE INDEX idx_tasks_search ON tasks USING GIN(
    to_tsvector('english', coalesce(name, '') || ' ' || coalesce(description, ''))
);

CREATE INDEX idx_workflows_search ON workflows USING GIN(
    to_tsvector('english', coalesce(name, '') || ' ' || coalesce(description, ''))
);
```

---

## ğŸ”’ çº¦æŸå’Œè§¦å‘å™¨

### 1. æ£€æŸ¥çº¦æŸ
```sql
-- çŠ¶æ€æœ‰æ•ˆæ€§çº¦æŸå·²åœ¨è¡¨å®šä¹‰ä¸­åŒ…å«

-- é¢å¤–çš„ä¸šåŠ¡é€»è¾‘çº¦æŸ
ALTER TABLE tasks ADD CONSTRAINT ck_tasks_retry_logic 
    CHECK (retry_count <= max_retries);

ALTER TABLE workflows ADD CONSTRAINT ck_workflows_version_format 
    CHECK (version ~ '^\d+\.\d+\.\d+$');
```

### 2. è§¦å‘å™¨å‡½æ•°
```sql
-- æ›´æ–°æ—¶é—´æˆ³è§¦å‘å™¨å‡½æ•°
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$ language 'plpgsql';

-- ä¸ºæ‰€æœ‰è¡¨åˆ›å»ºæ›´æ–°æ—¶é—´æˆ³è§¦å‘å™¨
CREATE TRIGGER update_agents_updated_at BEFORE UPDATE ON agents
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_tasks_updated_at BEFORE UPDATE ON tasks
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_workflows_updated_at BEFORE UPDATE ON workflows
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_executions_updated_at BEFORE UPDATE ON executions
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
```

### 3. ç»Ÿè®¡ä¿¡æ¯æ›´æ–°è§¦å‘å™¨
```sql
-- Agentæ‰§è¡Œç»Ÿè®¡æ›´æ–°å‡½æ•°
CREATE OR REPLACE FUNCTION update_agent_execution_stats()
RETURNS TRIGGER AS $$
BEGIN
    IF TG_OP = 'INSERT' AND NEW.execution_type = 'AGENT' THEN
        UPDATE agents SET total_executions = total_executions + 1
        WHERE id = NEW.agent_id;
    ELSIF TG_OP = 'UPDATE' AND NEW.execution_type = 'AGENT' THEN
        IF OLD.status != NEW.status THEN
            IF NEW.status = 'COMPLETED' THEN
                UPDATE agents SET 
                    successful_executions = successful_executions + 1,
                    avg_execution_time = (
                        avg_execution_time * (successful_executions - 1) + NEW.execution_time
                    ) / successful_executions
                WHERE id = NEW.agent_id;
            ELSIF NEW.status = 'FAILED' THEN
                UPDATE agents SET failed_executions = failed_executions + 1
                WHERE id = NEW.agent_id;
            END IF;
        END IF;
    END IF;
    RETURN COALESCE(NEW, OLD);
END;
$$ language 'plpgsql';

CREATE TRIGGER trigger_update_agent_stats
    AFTER INSERT OR UPDATE ON executions
    FOR EACH ROW EXECUTE FUNCTION update_agent_execution_stats();
```

---

## ğŸ”„ æ•°æ®è¿ç§»ç­–ç•¥

### 1. Alembicé…ç½®
```python
# alembic.ini é…ç½®
[alembic]
script_location = migrations
prepend_sys_path = .
version_path_separator = os
sqlalchemy.url = postgresql://user:password@localhost/crewai_studio

[post_write_hooks]
hooks = black
black.type = console_scripts
black.entrypoint = black
black.options = -l 79 REVISION_SCRIPT_FILENAME
```

### 2. åˆå§‹è¿ç§»è„šæœ¬
```python
# migrations/versions/001_initial_schema.py
"""Initial schema

Revision ID: 001
Revises: 
Create Date: 2024-01-01 00:00:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

revision = '001'
down_revision = None
branch_labels = None
depends_on = None

def upgrade():
    # åˆ›å»ºagentsè¡¨
    op.create_table('agents',
        sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('created_at', sa.TIMESTAMP(timezone=True), nullable=False),
        sa.Column('updated_at', sa.TIMESTAMP(timezone=True), nullable=False),
        sa.Column('name', sa.VARCHAR(255), nullable=False),
        sa.Column('description', sa.TEXT(), nullable=True),
        # ... å…¶ä»–å­—æ®µ
        sa.PrimaryKeyConstraint('id')
    )
    
    # åˆ›å»ºå…¶ä»–è¡¨...
    
    # åˆ›å»ºç´¢å¼•
    op.create_index('idx_agents_status', 'agents', ['status'])
    # ... å…¶ä»–ç´¢å¼•

def downgrade():
    op.drop_table('agents')
    # ... åˆ é™¤å…¶ä»–è¡¨
```

### 3. æ•°æ®è¿ç§»æœ€ä½³å®è·µ

#### è¿ç§»è„šæœ¬è§„èŒƒ
```python
# è¿ç§»è„šæœ¬æ¨¡æ¿
def upgrade():
    # 1. åˆ›å»ºæ–°è¡¨
    # 2. æ·»åŠ æ–°åˆ—
    # 3. æ•°æ®è½¬æ¢
    # 4. åˆ é™¤æ—§åˆ—
    # 5. åˆ›å»ºç´¢å¼•
    # 6. æ·»åŠ çº¦æŸ
    pass

def downgrade():
    # åå‘æ“ä½œ
    pass
```

#### å¤§æ•°æ®é‡è¿ç§»ç­–ç•¥
```python
# åˆ†æ‰¹å¤„ç†å¤§è¡¨è¿ç§»
def upgrade():
    # æ·»åŠ æ–°åˆ—
    op.add_column('large_table', sa.Column('new_column', sa.String(255)))
    
    # åˆ†æ‰¹æ›´æ–°æ•°æ®
    connection = op.get_bind()
    batch_size = 10000
    offset = 0
    
    while True:
        result = connection.execute(
            text(f"""
            UPDATE large_table 
            SET new_column = old_column 
            WHERE id IN (
                SELECT id FROM large_table 
                WHERE new_column IS NULL 
                LIMIT {batch_size}
            )
            """)
        )
        if result.rowcount == 0:
            break
        offset += batch_size
    
    # æ·»åŠ éç©ºçº¦æŸ
    op.alter_column('large_table', 'new_column', nullable=False)
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. æŸ¥è¯¢ä¼˜åŒ–
```sql
-- ä½¿ç”¨EXPLAIN ANALYZEåˆ†ææŸ¥è¯¢æ€§èƒ½
EXPLAIN ANALYZE SELECT * FROM tasks 
WHERE status = 'PENDING' AND priority > 5
ORDER BY created_at DESC LIMIT 10;

-- ä¼˜åŒ–å¸¸è§æŸ¥è¯¢
-- 1. åˆ†é¡µæŸ¥è¯¢ä¼˜åŒ–
SELECT * FROM tasks 
WHERE created_at < '2024-01-01' 
ORDER BY created_at DESC 
LIMIT 20;

-- 2. èšåˆæŸ¥è¯¢ä¼˜åŒ–
SELECT 
    status, 
    COUNT(*) as count,
    AVG(execution_time) as avg_time
FROM executions 
WHERE created_at >= CURRENT_DATE - INTERVAL '7 days'
GROUP BY status;
```

### 2. è¿æ¥æ± é…ç½®
```python
# SQLAlchemyè¿æ¥æ± é…ç½®
engine = create_engine(
    DATABASE_URL,
    pool_size=20,          # è¿æ¥æ± å¤§å°
    max_overflow=30,       # æœ€å¤§æº¢å‡ºè¿æ¥æ•°
    pool_timeout=30,       # è·å–è¿æ¥è¶…æ—¶æ—¶é—´
    pool_recycle=3600,     # è¿æ¥å›æ”¶æ—¶é—´
    pool_pre_ping=True,    # è¿æ¥å‰pingæµ‹è¯•
    echo=False             # ç”Ÿäº§ç¯å¢ƒå…³é—­SQLæ—¥å¿—
)
```

### 3. åˆ†åŒºç­–ç•¥
```sql
-- æŒ‰æ—¶é—´åˆ†åŒºexecutionsè¡¨
CREATE TABLE executions_partitioned (
    LIKE executions INCLUDING ALL
) PARTITION BY RANGE (created_at);

-- åˆ›å»ºæœˆåº¦åˆ†åŒº
CREATE TABLE executions_2024_01 PARTITION OF executions_partitioned
    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE executions_2024_02 PARTITION OF executions_partitioned
    FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');
```

---

## ğŸ” å®‰å…¨ç­–ç•¥

### 1. æ•°æ®åº“ç”¨æˆ·æƒé™
```sql
-- åˆ›å»ºåº”ç”¨ç”¨æˆ·
CREATE USER crewai_app WITH PASSWORD 'secure_password';

-- æˆäºˆå¿…è¦æƒé™
GRANT CONNECT ON DATABASE crewai_studio TO crewai_app;
GRANT USAGE ON SCHEMA public TO crewai_app;
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO crewai_app;
GRANT USAGE ON ALL SEQUENCES IN SCHEMA public TO crewai_app;

-- åˆ›å»ºåªè¯»ç”¨æˆ·
CREATE USER crewai_readonly WITH PASSWORD 'readonly_password';
GRANT CONNECT ON DATABASE crewai_studio TO crewai_readonly;
GRANT USAGE ON SCHEMA public TO crewai_readonly;
GRANT SELECT ON ALL TABLES IN SCHEMA public TO crewai_readonly;
```

### 2. æ•æ„Ÿæ•°æ®å¤„ç†
```sql
-- æ•æ„Ÿå­—æ®µåŠ å¯†å­˜å‚¨
-- ä½¿ç”¨pgcryptoæ‰©å±•
CREATE EXTENSION IF NOT EXISTS pgcrypto;

-- åŠ å¯†å­˜å‚¨ç¤ºä¾‹
CREATE TABLE secure_configs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    encrypted_value BYTEA, -- åŠ å¯†å­˜å‚¨çš„å€¼
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- æ’å…¥åŠ å¯†æ•°æ®
INSERT INTO secure_configs (name, encrypted_value) 
VALUES ('api_key', pgp_sym_encrypt('secret_api_key', 'encryption_key'));

-- æŸ¥è¯¢è§£å¯†æ•°æ®
SELECT name, pgp_sym_decrypt(encrypted_value, 'encryption_key') as value 
FROM secure_configs WHERE name = 'api_key';
```

---

## ğŸ“‹ å¤‡ä»½å’Œæ¢å¤

### 1. å¤‡ä»½ç­–ç•¥
```bash
#!/bin/bash
# æ•°æ®åº“å¤‡ä»½è„šæœ¬

DB_NAME="crewai_studio"
BACKUP_DIR="/backups/postgresql"
DATE=$(date +"%Y%m%d_%H%M%S")
BACKUP_FILE="${BACKUP_DIR}/${DB_NAME}_${DATE}.sql"

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# æ‰§è¡Œå¤‡ä»½
pg_dump -h localhost -U postgres -d $DB_NAME > $BACKUP_FILE

# å‹ç¼©å¤‡ä»½æ–‡ä»¶
gzip $BACKUP_FILE

# åˆ é™¤7å¤©å‰çš„å¤‡ä»½
find $BACKUP_DIR -name "${DB_NAME}_*.sql.gz" -mtime +7 -delete

echo "Backup completed: ${BACKUP_FILE}.gz"
```

### 2. æ¢å¤ç­–ç•¥
```bash
#!/bin/bash
# æ•°æ®åº“æ¢å¤è„šæœ¬

BACKUP_FILE=$1
DB_NAME="crewai_studio"

if [ -z "$BACKUP_FILE" ]; then
    echo "Usage: $0 <backup_file>"
    exit 1
fi

# åœæ­¢åº”ç”¨æœåŠ¡
sudo systemctl stop crewai-studio

# åˆ é™¤ç°æœ‰æ•°æ®åº“
dropdb -h localhost -U postgres $DB_NAME

# åˆ›å»ºæ–°æ•°æ®åº“
createdb -h localhost -U postgres $DB_NAME

# æ¢å¤æ•°æ®
if [[ $BACKUP_FILE == *.gz ]]; then
    gunzip -c $BACKUP_FILE | psql -h localhost -U postgres -d $DB_NAME
else
    psql -h localhost -U postgres -d $DB_NAME < $BACKUP_FILE
fi

# å¯åŠ¨åº”ç”¨æœåŠ¡
sudo systemctl start crewai-studio

echo "Database restored from: $BACKUP_FILE"
```

---

## ğŸ“ˆ ç›‘æ§å’Œç»´æŠ¤

### 1. æ€§èƒ½ç›‘æ§æŸ¥è¯¢
```sql
-- æ…¢æŸ¥è¯¢ç›‘æ§
SELECT 
    query,
    calls,
    total_time,
    mean_time,
    rows
FROM pg_stat_statements 
ORDER BY total_time DESC 
LIMIT 10;

-- è¡¨å¤§å°ç›‘æ§
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
FROM pg_tables 
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- ç´¢å¼•ä½¿ç”¨æƒ…å†µ
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes 
ORDER BY idx_scan DESC;
```

### 2. å®šæœŸç»´æŠ¤ä»»åŠ¡
```sql
-- å®šæœŸæ¸…ç†è¿‡æœŸæ•°æ®
DELETE FROM executions 
WHERE created_at < CURRENT_DATE - INTERVAL '90 days'
AND status IN ('COMPLETED', 'FAILED');

-- æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯
ANALYZE;

-- é‡å»ºç´¢å¼•ï¼ˆå¦‚éœ€è¦ï¼‰
REINDEX INDEX CONCURRENTLY idx_executions_created_at;

-- æ¸…ç†æ— ç”¨çš„æ‰§è¡Œæ—¥å¿—
UPDATE executions 
SET logs = '[]' 
WHERE created_at < CURRENT_DATE - INTERVAL '30 days'
AND jsonb_array_length(logs) > 100;
```

---

## ğŸ“ æ€»ç»“

æœ¬æ•°æ®åº“è®¾è®¡æ–‡æ¡£æä¾›äº† CrewAI Studio é¡¹ç›®çš„å®Œæ•´æ•°æ®åº“æ¶æ„ï¼ŒåŒ…æ‹¬ï¼š

1. **å®Œæ•´çš„æ•°æ®æ¨¡å‹å®šä¹‰** - æ¶µç›–æ‰€æœ‰æ ¸å¿ƒå®ä½“å’Œå…³ç³»
2. **ä¼˜åŒ–çš„ç´¢å¼•ç­–ç•¥** - æå‡æŸ¥è¯¢æ€§èƒ½
3. **å®Œå–„çš„çº¦æŸå’Œè§¦å‘å™¨** - ä¿è¯æ•°æ®å®Œæ•´æ€§
4. **çµæ´»çš„è¿ç§»ç­–ç•¥** - æ”¯æŒå¹³æ»‘çš„ç‰ˆæœ¬å‡çº§
5. **å…¨é¢çš„å®‰å…¨æªæ–½** - ä¿æŠ¤æ•æ„Ÿæ•°æ®
6. **å¯é çš„å¤‡ä»½æ¢å¤** - ç¡®ä¿æ•°æ®å®‰å…¨
7. **æŒç»­çš„ç›‘æ§ç»´æŠ¤** - ä¿æŒç³»ç»Ÿå¥åº·

è¯¥è®¾è®¡æ”¯æŒé«˜å¹¶å‘ã€å¤§æ•°æ®é‡çš„ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ï¼Œä¸º CrewAI Studio çš„ç¨³å®šè¿è¡Œæä¾›äº†åšå®çš„æ•°æ®åŸºç¡€ã€‚